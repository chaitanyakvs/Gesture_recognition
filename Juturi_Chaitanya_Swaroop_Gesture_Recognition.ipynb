{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement\n",
    "--------------------------------\n",
    "\n",
    "Imagine you are working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n",
    "\n",
    "The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "\n",
    "Thumbs up:  Increase the volume\n",
    "Thumbs down: Decrease the volume\n",
    "Left swipe: 'Jump' backwards 10 seconds\n",
    "Right swipe: 'Jump' forward 10 seconds  \n",
    "Stop: Pause the movie\n",
    " \n",
    "\n",
    "Each video is a sequence of 30 frames (or images). In the next couple of lectures, our subject matter expert Snehansu will walk you through the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sys import getsizeof\n",
    "import abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/home/chaitanya/Gesture_Recognition/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/home/chaitanya//Gesture_Recognition/Project_data/val.csv').readlines())\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proj_folder = '/home/chaitanya/Gesture_Recognition/Project_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, MaxPooling2D, Conv2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proj_folder = '/home/chaitanya/Gesture_Recognition/Project_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to plot the training and validation plot accuracies.\n",
    "# Need to mention the losses\n",
    "\n",
    "def plot(history):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "    axes[0].plot(history.history['loss'])\n",
    "    axes[0].plot(history.history['vald_loss'])\n",
    "    axes[0].plot(history.history['loss', 'vald_loss'])\n",
    "    \n",
    "    axes[1].plot(history.history['categorical_accuracy'])\n",
    "    axes[1].plot(history.history['vald_categorical_accuracy'])\n",
    "    axes[1].plot(history.history['categorical_accuracy', 'vald_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATOR:\n",
    "\n",
    "The generator should be able to take a batch of videos as input without any error. Steps like cropping, resizing and normalization should be performed successfully. Here we are going to pre-process the images with different dimensions, to create video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ef generator(source_path, folder_list, batch_size):\n",
    " #   print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "  #  img_idx = #create a list of image numbers you want to use for a particular video\n",
    "   # while True:\n",
    "    #    t = np.random.permutation(folder_list)\n",
    "     #   num_batches = # calculate the number of batches\n",
    "      #  for batch in range(num_batches): # we iterate over the number of batches\n",
    "       #     batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "        #    batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "         #   for folder in range(batch_size): # iterate over the batch_size\n",
    "          #      imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "           #     for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "            #        image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "             #       \n",
    "              #      #crop the images and resize them. Note that the images are of 2 different shape \n",
    "               #     #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                #    batch_data[folder,idx,:,:,0] = #normalise and feed in the image\n",
    "                 #   batch_data[folder,idx,:,:,1] = #normalise and feed in the image\n",
    "                  #  batch_data[folder,idx,:,:,2] = #normalise and feed in the image\n",
    "                    \n",
    "                #batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            #yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class ModelBuilder(metaclass=abc.ABCMeta):\n",
    "    def initialize_path(self, proj_folder):\n",
    "        self.train_doc = np.random.permutation(open(proj_folder + '/' + 'train.csv').readlines())\n",
    "        self.train_doc = np.random.permutation(open(proj_folder + '/' + 'val.csv').readlines())\n",
    "        self.train_path = proj_folder + '/' + 'train'\n",
    "        self.val_path = proj_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "# Initializing image properties\n",
    "    def initialize_image_properties(self,image_height=100, image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_channels=5\n",
    "        self.total_frames=30\n",
    "        \n",
    "# Initialising the frames  tp  sample ,batchsize & the generator function, epochs\n",
    "    \n",
    "\n",
    "    def initialize_hyperparams(self, frames_to_sample=30, batch_size=20, num_epochs=10):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            x = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "            for batch in range(num_batches):\n",
    "                batch_data, batch_labels=self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "\n",
    "        \n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx):\n",
    "                \n",
    "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
    "            \n",
    "                # Normalizing images\n",
    "                \n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "\n",
    "                #cropping of images\n",
    "                \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n",
    "                                #      (image_resized.shape[1], image_resized.shape[0])\n",
    "            \n",
    "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "    \n",
    "    \n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            #os.mkdr(model_name)\n",
    "             os.mkdir(model_name)\n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        \n",
    "        earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n",
    "        callbacks_list = [checkpoint, LR, earlystop]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def define_model(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL:\n",
    "\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use Conv3D and MaxPooling3D and not Conv2D and Maxpooling2D for a 3D convolution model. You would want to use TimeDistributed while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelConv3D1(ModelBuilder):\n",
    "    #def define_model()\n",
    "    \n",
    "    #class ModelConv3D1(ModelBuilder):\n",
    "    def define_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, (3, 3, 3), Padding = 'same',\n",
    "                         input_shape=(self.frames_to_sample,self.image_height, self_image_width, self.channels)))\n",
    "        \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        model.add(Conv3D(32, (2, 2, 2), Padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Conv3D(64, (2, 2, 2), Padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        model.add(Conv3D(128, (2, 2, 2), Padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='sotmax'))\n",
    "        \n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelConv3D1' object has no attribute 'val_doc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-8ac9fffb34b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconv_3d1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModelConv3D1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconv_3d1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProj_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mconv_3d1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_image_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconv_3d1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes_to_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconv_3d1_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_3d1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-eb7308ee1ae6>\u001b[0m in \u001b[0;36minitialize_path\u001b[0;34m(self, proj_folder)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_val_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Initializing image properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelConv3D1' object has no attribute 'val_doc'"
     ]
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(Proj_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=8,num_epochs=1)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "conv_3d1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample cropping\n",
    "\n",
    "test_generator=ModelConv3D1()\n",
    "test_generator.initialize_path(Proj_folder)\n",
    "test_generator.initialize_image_properties(image_height= 150, image_width = 150)\n",
    "test_generator.initialize_hyperparams(frame_to_sample=16,batch_size=3,num_epochs=1)\n",
    "x=test_generator.generator(test_generator.val_path,test_generator.val_doc,augment=True)\n",
    "batch_data, batch_labels=next(x)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "axes[0].imshow(batch_data[0, 15,:, :, :])\n",
    "axes[1].imshow(batch_data[0, 15,:, :, :])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_generator=ModelConv3D1()\n",
    "test_generator.initialize_path(Proj_folder)\n",
    "test_generator.initialize_image_properties(image_height= 150, image_width = 150)\n",
    "test_generator.initialize_hyperparams(frame_to_sample=16,batch_size=4,num_epochs=3)\n",
    "\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"total params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator=ModelConv3D1()\n",
    "test_generator.initialize_path(Proj_folder)\n",
    "test_generator.initialize_image_properties(image_height= 100, image_width = 100)\n",
    "test_generator.initialize_hyperparams(frame_to_sample=16,batch_size=8,num_epochs=3)\n",
    "\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"total params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator=ModelConv3D1()\n",
    "test_generator.initialize_path(Proj_folder)\n",
    "test_generator.initialize_image_properties(image_height= 160, image_width = 160)\n",
    "test_generator.initialize_hyperparams(frame_to_sample=16,batch_size=16,num_epochs=3)\n",
    "\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"total params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator=ModelConv3D1()\n",
    "test_generator.initialize_path(Proj_folder)\n",
    "test_generator.initialize_image_properties(image_height= 160, image_width = 160)\n",
    "test_generator.initialize_hyperparams(frame_to_sample=16,batch_size=32,num_epochs=3)\n",
    "\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"total params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As per my point of view, Based on the experiments \"Image resolution and number of frames in sequence\" have more impact on training time\n",
    "\n",
    "# My point of view is to chane the resolution for each and every experiment to anylyze the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
